"""Preprocess tweets by tokenizing, removing stopwords and stemming words"""

#Import----------------------------------------------------------------------------------------------------
import nltk
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import *
import string


#Function definition---------------------------------------------------------------------------------------
def tokenize(file_name): ##Could implement an algorithm for this
    """ Takes as input a file name. Tokenize the tweets separating them using nltk function. Return a list of tokens"""
    tokenizer = TweetTokenizer(strip_handles=True)
    tokens = []
    file = open(file_name, 'r')
    for line in file:
        tokens.extend(tokenizer.tokenize(line))
    file.close()
    return tokens

def preprocess(lst, file_name):
    """ Takes in a list of words. Removes stop words and punctuations. Stem the words. Returns a file of tokens"""
    PUNCTUATION = list(string.punctuation)
    STOPWORDS = set(list(stopwords.words('english')) + PUNCTUATION + ['RT'])  # RT for retweet
    stemmer = PorterStemmer()

    file = open(file_name, 'w')
    for w in lst:
        if w.lower() not in STOPWORDS:
            token = stemmer.stem(w)
            file.write(token + '\n')
    file.close()


def stem(word): ## Or implement an algorithm for this
    """Converting a word to its stem"""

#Run Program------------------------------------------------------------------------------------------------

lst1 = tokenize('1218_6_tweets.txt')
lst2 = tokenize('1219_6_tweets.txt')
lst3 = tokenize('1224_6_tweets.txt')
lst4 = tokenize('1227_6_tweets.txt')
lst5 = tokenize('1229_6_tweets.txt')
lst = lst1+ lst2 + lst3 + lst4 + lst5
preprocess(lst, 'all_preprocessedTweets.txt')
