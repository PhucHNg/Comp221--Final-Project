"""Preprocess tweets by tokenizing, removing stopwords and stemming words"""

# Import----------------------------------------------------------------------------------------------------
import nltk
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem.porter import *
import string


# Function definition---------------------------------------------------------------------------------------

def tokenize(file_name):  # Could implement an algorithm for this
    """ Takes as input a file name. Tokenize the tweets separating them using nltk function. Return a list of tokens"""
    tokenizer = TweetTokenizer(strip_handles=True)
    tokens = []
    file = open(file_name, 'r')
    for line in file:
        tokens.append(tokenizer.tokenize(line))
    file.close()
    return tokens


def preprocess(data):
    """ Takes in a list of lists of words.
    Removes stop words and punctuations.
    Stem the words.
    Returns a new list of lists of stemmed words"""
    PUNCTUATION = list(string.punctuation)
    STOPWORDS = set(list(stopwords.words('english')) + PUNCTUATION + ['rt'] + ['...'])  # RT for retweet
    stemmer = PorterStemmer()
    newData = []
    for sentence in data:
        newSen = []
        for word in sentence:
            if word.lower() not in STOPWORDS:
                #w = stemmer.stem(word)
                newSen.append(word)
        newData.append(newSen)
    return newData


# Run Program------------------------------------------------------------------------------------------------

if __name__ == '__main__':
    pre_data = tokenize("all_streaming_api_tweets.txt")
    data = preprocess(pre_data)
    for i in range(0,20):
        print(data[i])
        print(pre_data[i])


